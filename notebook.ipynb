{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlFglgY8pPco"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
        "!pip install langchain einops accelerate transformers bitsandbytes scipy\n",
        "!pip install xformers sentencepiece\n",
        "!pip install llama-index llama_hub\n",
        "!pip install sentence-transformers pypdf openai glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4bPsA16mZlw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import download_loader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "\n",
        "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
        "loader = PyMuPDFReader()\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "auth_token = \"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\"\n",
        "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
        "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
        "If you don't know the answer to a question, please don't share false information. <</SYS>>\n",
        "\"\"\"  # Llama2's official system prompt\n",
        "\n",
        "\n",
        "def model_tokenizer_embedder(model_name, auth_token):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name, cache_dir=\"./model/\", use_auth_token=auth_token\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        cache_dir=\"./model/\",\n",
        "        use_auth_token=auth_token,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "\n",
        "    embedding_llm = LangchainEmbedding(\n",
        "        HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    )\n",
        "\n",
        "    return tokenizer, model, embedding_llm\n",
        "\n",
        "\n",
        "\n",
        "tokenizer, model, embedding_llm = model_tokenizer_embedder(model_name, auth_token)\n",
        "\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024, llm=llm, embed_model=embedding_llm\n",
        ")\n",
        "set_global_service_context(service_context)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(directory):\n",
        "    documents = []\n",
        "    for item_path in glob(directory + \"*.pdf\"):\n",
        "        documents.extend(loader.load(file_path=item_path, metadata=True))\n",
        "    return documents\n",
        "\n",
        "documents = load_documents(\"./documents/\")\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "\n",
        "def get_response(prompt):\n",
        "    response = query_engine.query(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "AU5ISLXD6ENT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response(\"Who are starring in guntur kaaram?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahlEw2Fy7TQa",
        "outputId": "005ee660-fdb1-4340-a3e3-4576cf43c1ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the context information provided, the cast of Guntur Kaaram includes:\n",
            "\n",
            "1. Mahesh Babu\n",
            "2. Sreeleela\n",
            "3. Meenakshi Chaudhary\n",
            "4. Jagapathi Babu\n",
            "5. Ramya Krishna\n",
            "6. Jayaram\n",
            "7. Prakash Raj\n",
            "\n",
            "Please note that the cast list may not be exhaustive, and there may be other actors or crew members involved in the film who are not mentioned in the provided context.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}